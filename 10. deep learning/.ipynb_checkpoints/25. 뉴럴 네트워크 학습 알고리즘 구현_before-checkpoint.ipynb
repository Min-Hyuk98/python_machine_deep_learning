{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴럴 네트워크 학습 알고리즘 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules\n",
    "\n",
    "- 보통은 Keras로 하지만, 지금은 학습목적으로 tensorflow방식 그대로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 구조 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()  ## 상속자인 tf.keras.Model 클래스의 __init__ 해주기\n",
    "        self.flatten = tf.keras.layers.Flatten(input_shape=(28,28))  ## 28 X 28행렬데이터를 벡터형태로 flatten\n",
    "        self.dense1 = tf.keras.layers.Dense(32, activation='relu') ## 첫번째 layer에는 32개의 뉴런 사용\n",
    "        self.dense2 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.dense3 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.dense4 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.dense5 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "        \n",
    "    def call(self, x, trainig=None, mask=None):\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dense4(x)\n",
    "        return self.dense5(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습(train) 함수 구현\n",
    "\n",
    "- image : 입력값\n",
    "- labels : 출력값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, image, labels, loss_object, optimizer, train_loss, train_accuracy):\n",
    "    \n",
    "    # forwarding\n",
    "    with tf.GradientTape() as tape: ## 이 안에서 하는 연산은 모두 미분을 계산해놓음  -> 나중에 편리하게 뽑아쓸 수 있음\n",
    "        predictions = model(image) # batch_size=32. label종류=10가지 일때 --> (32 X 10) 의 형태로 나올 것.\n",
    "        loss = loss_object(labels, predictions) \n",
    "        \n",
    "    # backwarding\n",
    "    gradients = tape.gradient(loss, model.trainable_variables) # loss를 모든 trainable_variables로 미분\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # zip()  ->  gradients와 trainable_variables를 같이 iteration 할 수 있는 iteration 객체\n",
    "        \n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(model, image, labels, loss_object, test_loss, test_accuracy):\n",
    "    predictions = model(image) # batch_size=32. label종류=10가지 일때 --> (32 X 10) 의 형태로 나올 것.\n",
    "    loss = loss_object(labels, predictions) \n",
    "    \n",
    "    test_loss(loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1024).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실함수 및 최적화 알고리즘 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ... 5 6 8] (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train, y_train.shape) ## y의 범주들이 one-hot encoding되어있지 않고 각각이 범주이름을 나타낸다 --> SparseCategoricalCrossentropy 사용\n",
    "## y의 범주들이 one-hot encoding되어있으면 --> CategoricalCrossentropy 사용\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 성능 지표 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name=\"test_loss\")\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"test_accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 루프 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1014 20:18:21.086135 24680 base_layer.py:1814] Layer my_model_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss: 0.24200297892093658, Accuracy: 92.78500366210938, Test Loss: 0.13502158224582672, Test Accuracy: 95.84000396728516\n",
      "Epoch 2: loss: 0.10781527310609818, Accuracy: 96.59666442871094, Test Loss: 0.11546571552753448, Test Accuracy: 96.51000213623047\n",
      "Epoch 3: loss: 0.08006967604160309, Accuracy: 97.46166229248047, Test Loss: 0.09669629484415054, Test Accuracy: 97.23999786376953\n",
      "Epoch 4: loss: 0.06349801272153854, Accuracy: 97.95166778564453, Test Loss: 0.10453332960605621, Test Accuracy: 97.04000091552734\n",
      "Epoch 5: loss: 0.053288742899894714, Accuracy: 98.30500030517578, Test Loss: 0.0958881601691246, Test Accuracy: 97.38999938964844\n",
      "Epoch 6: loss: 0.04579765349626541, Accuracy: 98.48500061035156, Test Loss: 0.09782060235738754, Test Accuracy: 97.29000091552734\n",
      "Epoch 7: loss: 0.036919910460710526, Accuracy: 98.78666687011719, Test Loss: 0.1066141352057457, Test Accuracy: 97.48999786376953\n",
      "Epoch 8: loss: 0.03473953157663345, Accuracy: 98.90333557128906, Test Loss: 0.10619760304689407, Test Accuracy: 97.43000030517578\n",
      "Epoch 9: loss: 0.030143260955810547, Accuracy: 99.0, Test Loss: 0.12047237157821655, Test Accuracy: 97.30999755859375\n",
      "Epoch 10: loss: 0.028744257986545563, Accuracy: 99.0433349609375, Test Loss: 0.1167968288064003, Test Accuracy: 97.3699951171875\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for image, labels in train_ds:\n",
    "        train_step(model, image, labels, loss_object, optimizer, train_loss, train_accuracy)\n",
    "        \n",
    "    for image, labels in test_ds:\n",
    "        test_step(model, image, labels, loss_object, test_loss, test_accuracy)\n",
    "    \n",
    "    print('Epoch {}: loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'.format(epoch+1, \n",
    "                                                                                       train_loss.result(), \n",
    "                                                                                       train_accuracy.result() * 100, \n",
    "                                                                                       test_loss.result(), \n",
    "                                                                                       test_accuracy.result() * 100))\n",
    "    \n",
    "    ## 누적되는것을 방지하기 위해 reset해줌\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
